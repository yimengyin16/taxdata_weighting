---
title: "TaxData Enhancement Project"
author: Don Boyd
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook: 
    df_print: paged
    toc: yes
    toc_depth: 5
    number_sections: true
editor_options: 
  chunk_output_type: inline
---

# Project Statement of Work

> Augment Boydâ€™s current project in which he is developing methods for creating a single state microdata tax file, in two ways: (a) rigorously compare file quality under several different approaches under investigation, and (b) port the approach that produces the best results from R to Python so that it can readily be integrated with TaxData.


<!-- First set things up, then run things -->


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```


```{r system_specific_info, include=TRUE}
# change these as needed

pufdir <- "C:/Dropbox/AA_Projects_Work/Proj_taxData/PUF/"

# Note that my .Rprofile file runs these commands:
# RPROJ <- list(PROJHOME = normalizePath(getwd()))
# attach(RPROJ)
# rm(RPROJ)

# It does this because rmd files change the home directory.
# This creates a PROJHOME variable that points to the project main directory that can be used when saving and reading files.
# See how it is used in file reads below.


```


```{r libraries, include=FALSE}
library("magrittr")
library("plyr") # needed for ldply; must be loaded BEFORE dplyr
library("tidyverse")
options(tibble.print_max = 60, tibble.print_min = 60) # if more than 60 rows, print 60 - enough for states
# ggplot2 tibble tidyr readr purrr dplyr stringr forcats

library("scales")
library("hms") # hms, for times
library("lubridate") # lubridate, for date/times
library("vctrs")

library("grDevices")
library("knitr")
library("kableExtra")

# library("zoo") # for rollapply

# library("forecast")

library("ipoptr")

```



```{r general_functions, include=FALSE}
ns <- function(df) {names(df) %>% sort} # name sort

ht <- function(df, nrecs=6){
  print(utils::head(df, nrecs))
  print(utils::tail(df, nrecs))
}

```


# Make sure that `ipoptr` works by solving the simple "banana" test problem included with the package
Solve the Rosenbrock Banana test problem, a nonlinear function of 2 variables.

Don't worry now about reading the output. If you see "EXIT: Optimal Solution Found." then everything worked fine.

```{r echo=TRUE, message=FALSE, warning=FALSE}
## Rosenbrock Banana function
eval_f <- function(x) {   
  return( 100 * (x[2] - x[1] * x[1])^2 + (1 - x[1])^2 )
}

## Gradient of Rosenbrock Banana function
eval_grad_f <- function(x) { 
  return( c( -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
             200 * (x[2] - x[1] * x[1])) )
}

# The Hessian for this problem is actually dense, 
# This is a symmetric matrix, fill the lower left triangle only.
eval_h_structure <- list( c(1), c(1,2) )

eval_h <- function( x, obj_factor, hessian_lambda ) {
  return( obj_factor*c( 2 - 400*(x[2] - x[1]^2) + 800*x[1]^2,      # 1,1
                        -400*x[1],                                 # 2,1
                        200 ) )                                    # 2,2
}

# initial values
x0 <- c( -1.2, 1 )

opts <- list("print_level" = 5,
             "file_print_level" = 12,
             "output_file" = "banana.out",
             "tol"=1.0e-8)
opts$output_file <- paste0(PROJHOME, "/results/banana.out")

# solve Rosenbrock Banana function with analytic hessian 
print(ipoptr( x0=x0, 
               eval_f=eval_f, 
               eval_grad_f=eval_grad_f, 
               eval_h=eval_h,
               eval_h_structure=eval_h_structure,
               opts=opts) )

```


# Set things up so that we can test `ipoptr` on PUF targeting problems

## Create a PUF subset to work with
ONETIME: First, create a small PUF test data set from the PUF that we will work with. See chunk code for information.

```{r ONETIME_pufsub, eval=FALSE}
# You will have to run this one time on your own machine - set include=TRUE
# Note that I have the data directory in my .gitignore file so that the data - which are restricted in use - are not uploaded to github
pufraw <- read_csv(paste0(pufdir, "puf2011.csv"), col_types = cols(.default= col_double()))
# glimpse(pufraw)
ns(pufraw)

# keep a few variables:
# RECID unique id
# E00100 AGI
# E00200 wages
# E00300 interest received -- not kept
# E01700 pension income in AGI
# E02000 schedule E net income 
# S006 record weight as an integer - must be divided by 100
vars <- c("RECID", "E00100", "E00200", "E01700", "E02000", "S006")
set.seed(1234)
pufsub <- pufraw %>% 
  sample_n(5000) %>%
  select(vars) %>%
  mutate(weight=S006 / 100, otheragi=E00100 - E00200 - E01700 - E02000) %>%
  select(-S006) %>%
  select(RECID, weight, everything())

# glimpse(pufsub)
# ht(pufsub)

saveRDS(pufsub, paste0(PROJHOME, "/data/pufsub.rds")) # make sure data folder is in .gitignore

```


Get the PUF subset data and show its contents. We kept 5,000 records, selected randomly, with the following information:

+ RECID unique id
+ E00100 AGI
+ E00200 wages
+ E01700 pension income in AGI
+ E02000 schedule E net income (Rental real estate, royalties, partnerships, S corporations, trusts, etc.)
+ otheragi calculated as E00100 minus the other items
+ weight (the original S006 record weight, which we divided by 100 so that it is in the correct units)

```{r getpufsub}
pufsub <- readRDS(paste0(PROJHOME, "/data/pufsub.rds")) 
# glimpse(pufsub) - do not show individual records - they are restricted and the html file is uploaded to github
summary(pufsub)
```



## Define an objective function for use with PUF targeting
### What does an objective function look like?
We're going to choose new weights for the PUF that will achieve some targets (constraints). In these exaples, we want to choose weights that are close to the original weights on the file, but hit the targets. We'll penalize new weights that are far from the old weights.

We'll compute the ratio of the new weight to the current weight - call it x (a vector of weight adjustment multipliers, one for each record) and minimize the following objective (penalty) function based upon this ratio:

$$min \sum_{x} w_1(x^2 + x^{-2} -2)$$

where w1 is the original weight and x is the ratio of the new weight (call it w2) to the original weight (i.e., $x=w_2 / w_1$).

For any single record, the penalty function is minimized if the ratio, x, is 1. It penalizes proportionate differences from 1 equally. That is, if the weight is reduced to 10% of the original weight (x=.1) the penalty is the same as if the weight is increased to 10 times the original weight (x=10). Far distances from 1 are penalized disproportionately relative to smaller distances.

Because the penalty is weighted by the current weight, a change to the weight on a record that is very important (has a large weight) will have a greater penalty than a change to the weight on a record that is less important.

The minimum possible value for the function, summed over all x's, is zero of course.

The minus 2 is not necessary analytically but it makes the objective function easier to interpret and also reduces risk of numerical instability by keeping the objective function calculation for each weight close to zero.

The following graph shows the objective function for x ranging from 0.1 to 10, where w1 is assumed away (weight is 1 for all records). It is minimized at 1, of course, and the values at 0.1 and 10.0 are the same.

```{r plot_obj, echo=TRUE}
# graph of the objective  assuming all weights are 1.
f1 <- function(x) x^2 + x^-2 - 2
 
tibble(x=seq(0.01, 100, .01)) %>%
  mutate(y=f1(x)) %>%
  filter(x > 0.09, x < 10.01) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_vline(xintercept = 1, linetype="dashed") +
  scale_x_continuous(name="x (ratio of new weight to old weight)", 
                     breaks=c(.1, .25, .5, .75, 1, 2:20)) +
  theme(axis.text.x = element_text(angle = -90, vjust=0.5)) +
  scale_y_continuous(name="y (penalty)") +
  ggtitle("Objective function y=x^2 + x^-2 - 2, over the interval [0.1, 10]")


```

This is not the only possible objective function. Two other candidates are:

+ The TaxData objective function: minimize the sum of the absolute value of the percentage change from the original weight to the new weight (or, identically, minimize the sum of the difference between x and 1), or $min \sum_x abs(x - 1)$
+ An alternative that has been put forth $min \sum_x w_1(x - 1)^2$

Note that the function shown in the previous graph is symmetric around 1 in x and its multiplicative inverse, while both of these are additively symmetric around 1.

Note also that the TaxData function is not continuous or differentiable. Also, it is not weighted by the current weight.

```{r plot_altobj, echo=TRUE}

f2 <- function(x) abs(x - 1) # the TaxData function
f3 <- function(x) (x - 1)^2 # a continuous differentiable variant of the TaxData function

tibble(x=seq(0.01, 100, .01)) %>%
  filter(x > 0.09, x < 10.01) %>%
  mutate(y1=f1(x), y2=f2(x), y3=f3(x),
         # scale the functions to have the same value at x==10
         y2=y2 * y1[x==10] / y2[x==10],
         y3=y3 * y1[x==10] / y3[x==10]) %>%
  gather(objfn, y, y1, y2, y3) %>%
  mutate(objfn=factor(objfn, 
                      labels=c("y1: x^2 + x^-2 - 2",
                               "y2: abs(x - 1)",
                               "y3: (x - 1)^2"))) %>%
  ggplot(aes(x, y, colour=objfn)) +
  geom_line() +
  geom_vline(xintercept = 1, linetype="dashed") +
  scale_x_continuous(name="x (ratio of new weight to old weight)", 
                     breaks=c(.1, .25, .5, .75, 1, 2:20)) +
  theme(axis.text.x = element_text(angle = -90, vjust=0.5)) +
  scale_y_continuous(name="y (penalty)\nscaled so that all have the same penalty at x==10") +
  ggtitle("Selected objective functions over the interval [0.1, 10]")


```


### Implement an objective function and its gradient
Define R functions that will be used within ipoptr (and can be used externally, too) to calculate, for any given x vector:

+ The objective function evaluated at point x (a vector). It returns a single value.
+ The gradient of the objective function evaluated at point x. It returns a vector of values, the same length as x, where each element is the partial derivative of the objective function with respect to the corresponding element in x. This is used by IPOPT in choosing a search direction. While some non-linear programming (NLP) methods do not require derivatives, IPOPT does. While it is possible to write a function that calculates approximate derivatives, it is better to specify analytic derivatives when possible.

In addition to passing x to each function we will pass a list, that I call `inputs`, that can have anything we want it to have. The reason for this is that `ipoptr` requires any function it uses to receive the same arguments. Passing the same list to each function allows us to have functions that use different items (all contained within inputs) but receive the same arguments.

We could call these functions anything. I call the objective function `eval_f_xtop` which uses the name `ipoptr` uses for the objective (eval_f) plus a suffix, xtop, which stands for x to the power p (we could use any even power for the objective funciton, not just 2 as in the example above). Similarly, I call the gradient `eval_grad_f_xtop`.


```{r functions_obj, echo=TRUE}
eval_f_xtop <- function(x, inputs) {
  #.. objective function - evaluates to a single number ----
  # returns a single value
  
  # ipoptr requires that ALL functions receive the same arguments, so a list called inputs is passed to ALL functions
  
  # here are the objective function, the 1st deriv, and the 2nd deriv
  # http://www.derivative-calculator.net/
  # w{x^p + x^(-p) - 2}                                 objective function
  # w{px^(p-1) - px^(-p-1)}                             first deriv
  # p*w*x^(-p-2)*((p-1)*x^(2*p)+p+1)                    second deriv
  
  # make it easier to read:
  p <- inputs$p
  w <- inputs$weight
  
  obj <- sum(w * {x^p + x^(-p) -2})
  
  return(obj)
}


eval_grad_f_xtop <- function(x, inputs){
  #.. gradient of objective function - a vector length x ----
  # giving the partial derivatives of obj wrt each x[i]
  # returns one value per element of x
  
  # ipoptr requires that ALL functions receive the same arguments, so a list called inputs is passed to ALL functions
  
  # here are the objective function, the 1st deriv, and the 2nd deriv
  # http://www.derivative-calculator.net/
  # w{x^p + x^(-p) - 2}                                 objective function
  # w{px^(p-1) - px^(-p-1)}                             first deriv
  # p*w*x^(-p-2)*((p-1)*x^(2*p)+p+1)                    second deriv
  
  # make it easier to read:
  p <- inputs$p
  w <- inputs$weight
  
  gradf <- w * (p * x^(p-1) - p * x^(-p-1))
  
  return(gradf)
}
```


Calculate the objective function and its gradient for a few values, as a test:

```{r echo=TRUE}
x0 <- c(1, .1, 10)
inputs <- list()
inputs$p <- 2

inputs$weight <- 1
for(i in 1:length(x0)) {print(eval_f_xtop(x0[i], inputs))}

inputs$weight <- c(1, 1, 1)
eval_f_xtop(x0, inputs)
eval_grad_f_xtop(x0, inputs)

```

<!-- 
https://support.rstudio.com/hc/en-us/community/posts/239333188-Error-creating-notebook-object-not-found
"inline R chunks that reference data that is not available when the R session is saved will not work"
devtools::install_github('rstudio/rmarkdown')
-->

## Define constraints and associated functions
We will define a set of targets for the reweighted PUF, or "constraints" in NLP nomenclature.

Examples of constraints are:

+ The sum of weighted adjusted gross income (E00100) or another variable in the file using a given set of weights. For example:
++ sum(E00100 * weight) gives the sum with current weights, and
++ sum(E00100 * weight * x) gives the sum with new weights, if x is the ratio of new weights to original weights
+ The number of weighted tax returns in the file
+ The sum of weighted negative values for a variable. For example, E02000 (rental income etc.) can be negative (a loss). We might want to target the sum of these negative values separately from the sum of positive values for the variable.
+ The weighted number of negative values for a variable (e.g., the number of returns, weighted, that have losses).
+ Any of the above, within specific segments of the file, such as the number returns with negative rental income, within the subset of the file that has married couples with income between $50k and $75k. Obviously it only makes sense to construct a target for such a narrow portion of the file if we think we can estimate or forecast what a proper target should be.

The file with existing weights has total AGI of `r scales::comma(sum(pufsub$E00100 * pufsub$weight))` and total wages of `r scales::comma(sum(pufsub$E00200 * pufsub$weight))`. We might (if we had information that suggested this), set a target for AGI that is 10% greater (`r scales::comma(sum(pufsub$E00100 * pufsub$weight)*1.1)`) while at the same time setting a target for wages that is 5% less (`r scales::comma(sum(pufsub$E00200 * pufsub$weight)*.95)`). 


### Creating a set of constraint coefficients using dense matrices
We need to tell `ipoptr` (which tells IPOPT) how the targeted values on our reweighted file will change as we change our variable x -- that is we need to define the first partial derivative of each constraint with respect to each x variable. In our problem, these are constants, so I call them constraint coefficients.

There is more than one way to set up the constraints for the problem. For now, not worrying about computer resources, we'll use dense matrices. Imagine a matrix where every row is a record in our data file, every column represents a possible constraint of interest (e..g, the weighted sum of AGI, or the weighted number of returns with negative rental income), and every cell in the matrix represents how much the constraint (e.g,. the weighted sum of AGI) will change if we change the ratio x applicable to that return.

Suppose that the j-th column of this matrix represents the constraint for the weighted sum of positive AGI; let's name it E00100_sumpos.
Suppose that the i-th row of this matrix, tax return i, has AGI of $10,000 and an original weight of 17. Then cell [i, j] in this matrix would be 10,000 x 17 or 170000 ($170k), which is how much aggregate AGi would increase if we increased x[i] by 1. For example, if we increased the weight for return i by 10% (x[i] is 1.1 and the change in x[i] is 0.1), total weighted AGI would increase by 10,000 x 17 x 10%.


#### Define functions to help in developing constraint coefficients
The functions below calculate constraint coefficients for common constraints of interest. var is a vector of values for a variable -- for example, var[i] in the example above is 10,000 and weight[i] is 17

```{r functions_concoef, echo=TRUE}
# functions to compute constraint coefficients
nnz <- function(var, weight) {(var != 0) * weight} # weighted number of returns with nonzero values
nz <- function(var, weight) {(var == 0) * weight} #  weighted number of returns with zero values
npos <- function(var, weight) {(var > 0) * weight} # weighted number of returns with positive values
nneg <- function(var, weight) {(var < 0) * weight} # weighted number of returns with negative values

sumval <- function(var, weight) {weight * var} # weighted sum of nonzero value
sumpos <- function(var, weight) {(var > 0) * weight * var}
sumneg <- function(var, weight) {(var < 0) * weight * var}

# sum(pufsub$E00100 * pufsub$weight) / 1e9

```


#### Create a constraint-coefficient matrix-like data frame
Now we apply these functions to our data to construct constraint coefficients for potential constraints, for a few selected variables. This is not necessarily the most efficient way to do this. For example we might want to first decide which constraints we care about, and only compute them. With small datasets, this is not an important consideration.

Each column will represent a constraint, and each row will correspond to a row in the data.

```{r echo=TRUE}
ccvars <- c("E00100", "E00200", "E02000")
ccoef_all <- pufsub %>%
  mutate_at(vars(ccvars), 
            list(npos = ~npos(., weight),
                 sumval = ~sumval(., weight),
                 nneg = ~nneg(., weight)))
# glimpse(ccoef_all)
ht(ccoef_all)

```


### Define generic function to calculate constraint sums based on a given data file and an x vector
```{r echo=TRUE}
calc_constraints <- function(ccoef, constraint_vars, x=rep(1, nrow(ccoef))){
  # calculated weighted sums of constraint_vars that are in data, using an optional multiplier x
  # weights: numeric vector
  # uwccoef: df with 1 column per constraint variable (plus possibly additional columns); 1 row per person (tax return), 
  #        values are constraint unweighted coefficients
  # contraint_vars: character vector
  # return: a named vector
  colSums(x * select(ccoef, constraint_vars))
}

```


Check: Print sums on file as it exists, and sums with alternative weights
```{r echo=TRUE}
# calculate values on the file and for some randomly chosen targets
vars <- str_subset(names(ccoef_all), "_")
filesums <- calc_constraints(ccoef_all, vars) # a named vector
# check:
# sum(pufsub$weight * pufsub$E00100)
# sum(ccoef_all$E00100_sumval)

# pick some random x's
set.seed(5)
x <- rnorm(nrow(pufsub), mean=1, sd=.2)
targets <- calc_constraints(ccoef_all, vars, x)

cbind(filesums, targets, gapratio=targets/filesums)
```


### Define constraint functions for use within `ipoptr`

`ipoptr` requires two functions and a list in relation to constraints:

* A function to calculate constraint values at point x. For a given vector x it returns a vector with one value for each constraint evaluated at point x. It is based on `calc_constraints` above but also accepts `inputs` as an argument and removes names from the result vector. Internally, ipoptr calls this function `eval_g`; I name our version `eval_g_dense` because it uses a dense matrix (or data frame) as input and in the calculation, rather than a sparse matrix.

* A function to calculate the Jacobian of the constraints - the first partial derivatives of each constraint with respect to each element of the vector x, evaluated at point x. `ipoptr` calls the Jacobian function `eval_jac_g` internally, and I have named our version `eval_jac_g` as well.

    + In concept the Jacobian is a matrix where the columns are the constraints, the rows are the variables (the elements of x), and the cells are the partial derivatives.  

    + The number of potential partial derivatives is the number of variables (the number of elements in x) times the number of constraints. In a problem with 5,000 records and 10 constraints the return vector will have 50,000 elements. In some problems many of those elements may be zero in which case sparse matrix methods can be useful. (Incidentally, we do not necessarily have to create the full Jacobian matrix. In a huge problem we might create only the nonzero elements. In our problem here, we don't worry about that and we create the full matrix even if we later create a sparse version with only the nonzero elements.)
   
    + `ipoptr` needs the Jacobian function to return the elements of the Jacobian matrix as a single long vector. The vector can contain either ALL elements of the Jacobian (whether zero or not - i.e., treating the Jacobian as a dense matrix), or just the nonzero elements of the Jacobian (treating it as a sparse matrix). The vector is ordered so that first it contains the elements of the first column (the first constraint), then the elements of the second column, and so on.
    
    + The helper function `jac_flatten` below flattens a Jacobian matrix into a long vector, in the order required by `ipoptr`. In its default mode it returns the entire flattened matrix. If the argument nz=TRUE, it will return only the nonzero elements, for sparse matrix methods discussed later.
    
    + We also have to give `ipoptr` information that tells it which elements of the vector correspond to which cells in the Jacobian. 
   
    + In our problem (but not all NLP problems) these derivatives are constant (do not depend on x); in other problems they may need to be computed in each iteration of the solver. Because our Jacobian is constant, we define it before calling `ipoptr` and simply return it unchanged each time our Jacobian function is called within `ipoptr`.
   
* A list that tells `ipoptr` the structure of the Jacobian - which elements of the vector returned from the Jacobian calculation function correspond to which cells in the Jacobian matrix.

    + We need this list whether we are returning a vector with the full dense Jacobian (as I do in the example below) or whether we are returning just the nonzero elements (a sparse approach, shown later.)
    
    + The list has one element for each constraint. Each such element is a vector specifying which rows of the Jacobian matrix are included in the vector (which elements of x enter into the calculation of this constraint).
    
    + The helper function below, `define_jac_g_structure_dense`, creates the list that `ipoptr` wants for a dense Jacobian, where every row (every element of x) is used in the calculation of every constraint (every column). The example below runs this function for a case of 2 constraints and 4 variables - the list has 2 elements, and each element has the vector (1, 2, 3, 4).
    
    + Later we'll construct an example with a sparse matrix, where we only specify the nonzero elements of the Jacobian.


```{r functions_ipopt, echo=TRUE}

eval_g_dense <- function(x, inputs){
  #.. constraints that must hold in the solution ----
  # just give the LHS of the expression
  # return a vector where each element evaluates a constraint (i.e., sum of (x * a cc matrix column), for each column)
  
  # ipoptr requires that ALL functions receive the same arguments, so a list called inputs is passed to ALL functions
  
  # This is the dense version in that it uses a dense set of constraint coefficients, inputs$ccoef (passed to this
  # function as a data frame but we could easily use a matrix instead). It has:
  #   one column for every constraint
  #   one row for every variable
  # Many of the cells are likely to be zero.
  
  unname(calc_constraints(inputs$ccoef, inputs$constraint_vars, x)) # ipoptr doesn't seem to like a named vector
}


eval_jac_g <- function(x, inputs){
  # Jacobian of the constraints ----
  # the Jacobian is the matrix of first partial derivatives of constraints (these derivatives may be constants)
  # this function evaluates the Jacobian at point x
  # It returns a vector with these partial derivatives
  
  # This is the dense version that returns a vector with one element for EVERY item in the Jacobian (including zeroes)
  # Thus the vector has n_constraints * n_variables elements
  # first, all of the elements for constraint 1, then all for constraint 2, etc...
  
  # because constraints in this problem are linear, the derivatives are all constants
  
  # ipoptr requires that ALL functions receive the same arguments, so the inputs list is passed to ALL functions
  
  return(inputs$jac_vector)
}


jac_flatten <- function(jac_df, nz=FALSE) {
  # flatten the Jacobian matrix 
  jac_vector <- c(as.matrix(jac_df))
  if(nz) jac_vector <- jac_vector[jac_vector != 0]
  return(jac_vector)
} 
# jac_flatten(inputs$ccoef)[1:30]
# jac_flatten(inputs$ccoef, nz=TRUE)[1:30]


define_jac_g_structure_dense <- function(n_constraints, n_variables){
  #.. function to define the structure of the Jacobian ---
  # list with n_constraints elements
  # each is 1:n_variables
  lapply(1:n_constraints, function(n_constraints, n_variables) 1:n_variables, n_variables)
} 
# Examples of how to use define_jac_g_structure_dense:
define_jac_g_structure_dense(n_constraints=2, n_variables=4)

# eval_jac_g_structure_dense <- define_jac_g_structure_dense(
#   n_constraints=ncol(data[, names(constraints)]), 
#   n_variables=nrow(data[, names(constraints)]))


```


### Define Hessian function for use within `ipoptr`

`ipoptr` does not require a Hessian matrix of 2nd derivatives but in this problem it generally works best with one.

When using the Hessian we have to provide two things:

* A function that computes the Hessian at point x. `ipoptr` calls this function `eval_h` internally and I call our vesion `eval_h_xtop` to reflect our objective function, which raises x to a power p. The function returns a vector of nonzero values in the Hessian matrix.

* A list that defines the Hessian structure - how the vector returned from the Hessian function relates to the cells of the Hessian matrix. 

    + In our case we only need the diagonal of the Hessian, and so the first element has the value 1, the second list element has the value 2, the third list element has the value 3, and so on. There is one element for each variable (each element of x). If x has length 5,000, then the list will have 5,000 elements and that 5,000th element will be 5000.
    
    + The helper function `hess_structure` below creates such a list. The example below shows the output of the function for a problem with 5 variables. CAUTION: This function is specific to our problem. Other problems (other than weighting microdata files) might use off-diagonal elements of the Hessian.

Calculating derivatives properly is crucial or IPOPT will not produce good results. Because my calculus is a little rusty, I check my derivative calculations against a website that can do this http://www.derivative-calculator.net/. IPOPT also has a built-in derivative checker that can be turned on with an IPOPT option (passed as an argument to the `ipoptr` options list). It calculates approximate derivatives by finite differences and compares them to the values returned from your derivative functions. It is extremely slow, but useful to use it as a check (limiting the number of iterations to 1 or 2) the first time you use a derivative function or when troubleshooting.


```{r hessian, echo=TRUE}
eval_h_xtop <- function(x, obj_factor, hessian_lambda, inputs){
  # The Hessian matrix ----
  # The Hessian matrix has many zero elements and so we set it up as a sparse matrix
  # We only keep the (potentially) non-zero values that run along the diagonal.
  # the Hessian is returned as a long vector. Separately, we define which elements of this vector correspond to which cells of the Hessian matrix.
  
  # obj_factor and hessian_lambda are required arguments of the function. They are created within ipoptr - we do not create them.
  
  # http://www.derivative-calculator.net/
  # w{x^p + x^(-p) - 2}                                 objective function
  # w{px^(p-1) - px^(-p-1)}                             first deriv
  # p*w*x^(-p-2)*((p-1)*x^(2*p)+p+1)                    second deriv
  
  # make it easier to read:
  p <- inputs$p
  w <- inputs$weight
  
  hess <- obj_factor * 
    (  p*w*x^(-p-2) * {(p-1)*x^(2*p) + p + 1}  )
  
  return(hess)
}

hess_structure <- function(nvariables) {lapply(1:nvariables, function(x) x)} # diagonal elements of our Hessian
hess_structure(nvariables=5)

```


# Start with a simple PUF targeting problem and examine progressively more complex problems
Later we will move to more-complex problems.

Here we look at a subset of the constraints we computed above. We compute current file totals and make up some perturbed totals that we will use as targets. Then we solve the problem.


First we define the targets and then solve the problem.

Remind ourselves of the constraint coefficients available: `r names(ccoef_all) %>% sort`
Let's target:

+ The number of returns -- E00100_npos will do that
+ Sum of AGI -- E00100_sumval
+ Sum of wages -- E00200_sumval
+ number of returns that have negative rental income -- E02000_nneg


```{r targets1, echo=TRUE}
# glimpse(ccoef_all)
# Let's target the number of returns (E00100_npos) will do that
# define constraint variables and their targets
# let's just use the sums

constraint_vars <- c("E00100_npos", "E00100_sumval", "E00200_sumval", "E02000_nneg")

ccoef <- ccoef_all %>%
  select(constraint_vars)
  
# Get the file sums so we know where we are starting from
(filesums <- calc_constraints(ccoef, constraint_vars))
# (filesums <- calc_constraints(ccoef, constraint_vars, x=1))

# create targets with perturbed values
targ_factors <- list(E00100_npos=1.25,
                     E00100_sumval=0.75,
                     E00200_sumval=.9,
                     E02000_nneg=.8) %>% 
  unlist # define multipliers for some values, as a named vector

targets <- filesums[names(targ_factors)] * targ_factors

# Look at our targets
cbind(init_vals=filesums[names(targets)], targets, ratio=targets/filesums[names(targets)])

```


## Solve a problem with no bounds on the x values
```{r optimize1_nobounds, echo=TRUE}
# glimpse(pufsub)
# glimpse(ccoef)
# targets

inputs <- list()
inputs$p <- 2
inputs$weights <- pufsub$weight
inputs$ccoef <- ccoef
inputs$constraint_vars <- constraint_vars
inputs$jac_vector <- jac_flatten(inputs$ccoef)

# set initial values of x
x0 <- rep(1, length(inputs$weights))

# create vectors with constraint lower bounds and upper bounds - if they are the same, we have equality constraints
clb <- targets[inputs$constraint_vars]
cub <- targets[inputs$constraint_vars]

cbind(init_vals=filesums[names(targets)], targets, clb, cub)

eval_jac_g_structure_dense <- define_jac_g_structure_dense(n_constraints=length(inputs$constraint_vars), 
                                                           n_variables=nrow(inputs$ccoef))
# str(eval_jac_g_structure_dense)

eval_h_structure <- hess_structure(length(inputs$weight))

# note that we do not display output to the screen (print_level = 0) but
# we do write output to a file (output_file = prob1.out)
opts <- list("print_level" = 0, # use a positive integer to see progress of IPOPT
             "file_print_level" = 5, # integer
             "linear_solver" = "ma27", # mumps pardiso ma27 ma57 ma77 ma86 ma97
             "max_iter"=200)
opts$output_file <- paste0(PROJHOME, "/results/prob1.out")
  
result <- ipoptr(x0 = x0,
                 eval_f = eval_f_xtop, # arguments: x, inputs
                 eval_grad_f = eval_grad_f_xtop,
                 eval_g = eval_g_dense, # constraints LHS - a vector of values
                 eval_jac_g = eval_jac_g,
                 eval_jac_g_structure = eval_jac_g_structure_dense,
                 eval_h = eval_h_xtop, # the hessian is essential for this problem
                 eval_h_structure = eval_h_structure,
                 constraint_lb = clb,
                 constraint_ub = cub,
                 opts = opts,
                 inputs = inputs)


names(result) %>% sort
result$status; result$message; result$iterations
result$objective
quantile(result$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

calcsums <- calc_constraints(inputs$ccoef, inputs$constraint_vars, x=result$solution)
cbind(filesums, targets, clb, calcsums, cub)

result1 <- result

```


## Rerun with bounds on the x values
```{r optimize2_bounds, echo=TRUE}
# only change what needs to be changed
 
# set arbitrary bounds for x that fall inside the solution above
xlb <- rep(.3, length(inputs$weights))
xub <- rep(10, length(inputs$weights))

opts$output_file <- paste0(PROJHOME, "/results/prob2.out")
opts
  
result <- ipoptr(x0 = rep(1, length(inputs$weights)),
                 # define the lower and upper bounds on the x variables
                 lb = xlb,
                 ub = xub,
                 eval_f = eval_f_xtop, # arguments: x, inputs
                 eval_grad_f = eval_grad_f_xtop,
                 eval_g = eval_g_dense, # constraints LHS - a vector of values
                 eval_jac_g = eval_jac_g,
                 eval_jac_g_structure = eval_jac_g_structure_dense,
                 eval_h = eval_h_xtop, # the hessian is essential for this problem
                 eval_h_structure = eval_h_structure,
                 constraint_lb = clb,
                 constraint_ub = cub,
                 opts = opts,
                 inputs = inputs)

# names(result) %>% sort
result$status; result$message; result$iterations
result$objective
quantile(result$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

calcsums <- calc_constraints(inputs$ccoef, inputs$constraint_vars, x=result$solution)
cbind(filesums, targets, clb, calcsums, cub)
result2 <- result

# compare objective function values - the value increased
result1$objective; result2$objective
# compare iterations - it took more this time
result1$iterations; result2$iterations


```


## Add targets for a portion of the distribution of wages
This is not necessarily the most efficient way to do this, but it should be pretty clear.

First, prepare the additional constraints.

```{r prepare_wages, echo=TRUE}
# add constraint coefficients for wages
glimpse(ccoef_all)
wagevals <- ccoef_all %>%
  select(RECID, weight, E00200) %>%
  mutate(wagegroup=cut(E00200, c(-Inf, 0, 10e3, 50e3, 100e3, Inf), labels=FALSE),
         wagegroup=paste0("wage", wagegroup),
         wtdwage=E00200 * weight) %>%
  spread(wagegroup, wtdwage) %>%
  mutate_at(vars(starts_with("wage")), list(~ifelse(is.na(.), 0, .))) %>%
  select(RECID, starts_with("wage"))
ht(wagevals)
# count(wagevals, wagegroup)

constraint_vars <- c("E00100_npos", "E00100_sumval", "E00200_sumval", "E02000_nneg", "wage1", "wage2", "wage3")

ccoef <- ccoef_all %>%
  left_join(wagevals) %>%
  select(constraint_vars)
  
# Get the file sums so we know where we are starting from
(filesums <- calc_constraints(ccoef, constraint_vars))
# (filesums <- calc_constraints(ccoef, constraint_vars, x=1))

# create targets with perturbed values
targ_factors <- list(E00100_npos=1.25,
                     E00100_sumval=0.75,
                     E00200_sumval=.9,
                     E02000_nneg=.8,
                     wage1=1,
                     wage2=.85,
                     wage3=.95) %>% 
  unlist # define multipliers for some values, as a named vector

targets <- filesums[names(targ_factors)] * targ_factors

# Look at our targets
cbind(init_vals=filesums[names(targets)], targets, ratio=targets/filesums[names(targets)])

targets

```

Now set up the optimization problem. With new constraints, define everything again to be on the safe side.

```{r optimize3_wages, echo=TRUE}
inputs <- list()
inputs$p <- 2
inputs$weights <- pufsub$weight
inputs$ccoef <- ccoef
inputs$constraint_vars <- constraint_vars
inputs$jac_vector <- jac_flatten(inputs$ccoef)

# create vectors with constraint lower bounds and upper bounds - if they are the same, we have equality constraints
clb <- targets[inputs$constraint_vars]
cub <- targets[inputs$constraint_vars]

cbind(init_vals=filesums[names(targets)], targets, clb, cub)

eval_jac_g_structure_dense <- define_jac_g_structure_dense(n_constraints=length(inputs$constraint_vars), 
                                                           n_variables=nrow(inputs$ccoef))
# str(eval_jac_g_structure_dense)

eval_h_structure <- hess_structure(length(inputs$weight))


# set arbitrary bounds for x that fall inside the solution above
xlb <- rep(.3, length(inputs$weights))
xub <- rep(10, length(inputs$weights))

opts <- list("print_level" = 0,
             "file_print_level" = 5, # integer
             "linear_solver" = "ma27", # mumps pardiso ma27 ma57 ma77 ma86 ma97
             "max_iter"=200)
opts$output_file <- paste0(PROJHOME, "/results/prob3.out")
  

result <- ipoptr(x0 = rep(1, length(inputs$weights)),
                 lb = xlb,
                 ub = xub,
                 eval_f = eval_f_xtop, # arguments: x, inputs
                 eval_grad_f = eval_grad_f_xtop,
                 eval_g = eval_g_dense, # constraints LHS - a vector of values
                 eval_jac_g = eval_jac_g,
                 eval_jac_g_structure = eval_jac_g_structure_dense,
                 eval_h = eval_h_xtop, # the hessian is essential for this problem
                 eval_h_structure = eval_h_structure,
                 constraint_lb = clb,
                 constraint_ub = cub,
                 opts = opts,
                 inputs = inputs)

result$status; result$message; result$iterations
result$objective
quantile(result$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

calcsums <- calc_constraints(inputs$ccoef, inputs$constraint_vars, x=result$solution)
cbind(filesums, targets, clb, calcsums, cub)
result3 <- result

# compare objective function values - the value increased
result1$objective; result2$objective; result3$objective
# compare iterations - it took more this time
result1$iterations; result2$iterations; result3$iterations


```


## Use inequality constraints
```{r optimize4_inequality, echo=TRUE}

# create vectors with constraint lower bounds and upper bounds
# define tolerance - for simplicity, use the same for all, but that is not necessary
tol <- .01 # 1% range
clb <- targets[inputs$constraint_vars]
clb <- clb - tol * abs(clb)

cub <- targets[inputs$constraint_vars]
cub <- cub + tol * abs(cub)

opts$output_file <- paste0(PROJHOME, "/results/prob4.out")
opts
  
result <- ipoptr(x0 = rep(1, length(inputs$weights)),
                 lb = xlb,
                 ub = xub,
                 eval_f = eval_f_xtop, # arguments: x, inputs
                 eval_grad_f = eval_grad_f_xtop,
                 eval_g = eval_g_dense, # constraints LHS - a vector of values
                 eval_jac_g = eval_jac_g,
                 eval_jac_g_structure = eval_jac_g_structure_dense,
                 eval_h = eval_h_xtop, # the hessian is essential for this problem
                 eval_h_structure = eval_h_structure,
                 # the constraint bounds are given here
                 constraint_lb = clb,
                 constraint_ub = cub,
                 opts = opts,
                 inputs = inputs)

result$status; result$message; result$iterations
result$objective
quantile(result$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

calcsums <- calc_constraints(inputs$ccoef, inputs$constraint_vars, x=result$solution)

cbind(filesums, targets, clb, calcsums, cub) %>%
  kable(digits=0, format.args = list(big.mark=","))

result4 <- result

# compare objective function values
result1$objective; result2$objective; result3$objective; result4$objective
# compare iterations
result1$iterations; result2$iterations; result3$iterations; result4$iterations

```



## Use scaling options
Set objective function scaling (read about this in IPOPT options), as our objective function can be very large. Add the options to the list called opts.

Notice that this takes fewer iterations and less time.

```{r optimize5_wages_scaled, echo=TRUE}

opts$output_file <- paste0(PROJHOME, "/results/prob5.out")
opts$obj_scaling_factor <- 1e-4 # default 1 
opts$nlp_scaling_max_gradient <- 100 # default 100 -- not changing this now
opts

result <- ipoptr(x0 = rep(1, length(inputs$weights)),
                 lb = xlb,
                 ub = xub,
                 eval_f = eval_f_xtop, # arguments: x, inputs
                 eval_grad_f = eval_grad_f_xtop,
                 eval_g = eval_g_dense, # constraints LHS - a vector of values
                 eval_jac_g = eval_jac_g,
                 eval_jac_g_structure = eval_jac_g_structure_dense,
                 eval_h = eval_h_xtop, # the hessian is essential for this problem
                 eval_h_structure = eval_h_structure,
                 constraint_lb = clb,
                 constraint_ub = cub,
                 opts = opts,
                 inputs = inputs)

result$status; result$message; result$iterations
result$objective
quantile(result$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

calcsums <- calc_constraints(inputs$ccoef, inputs$constraint_vars, x=result$solution)

cbind(filesums, targets, clb, calcsums, cub) %>%
  kable(digits=0, format.args = list(big.mark=","))

result5 <- result

# compare objective function values
result1$objective; result2$objective; result3$objective; result4$objective; result5$objective

# compare iterations - scaling was a big improvement
result1$iterations; result2$iterations; result3$iterations; result4$iterations; result5$iterations

```


## Use sparse matrices -- converting dense to sparse
Some NLP problems have many constraint coefficients that are zero. For example, suppose we have 100,000 variables (100,000 elements in x) and we have 10 constraints, 1 each for the sum of wages in each of 10 income ranges. Suppose further, for convenience, that 10% of returns fall in each income range. Let's say the first income range is AGI from 0 to $25,000. If we adjust weights for the 10% of records that fall in this range, then that will adjust the total of weighted wages in the range. But no matter how we vary weights on the other 90% of records, we cannot change the total of weighted wages in the range. The same is true for each of the other income ranges. Thus, in this example, only 10% of records can affect wages in any of the income ranges -- only 10% of records have nonzero constraint coefficients for any given constraint.

If we store these constraints in a regular matrix with 10 columns (1 for each constraint) and 100,000 rows, it will have 1 million elements even though only 100,000 of them affect constraints. Another way of storing the matrix is to store just the nonzero elements in a big long vector, and keep track of which cells in the matrix correspond to which items in the vector of nonzero values.

The function below, `make.sparse.structure`, takes a dense matrix and returns a list with its sparsity structure, for passing to `ipoptr`.

```{r echo}
make.sparse.structure <- function(ccoef) {
  # argument is a constraint coefficients data frame (or could be a matrix)
  #  -- columns are constraints
  #  -- rows are variables
  # return a list with indexes for nonzero constraint coefficients
  
  # this is much faster than the make.sparse function in ipoptr
  
  f <- function(x) which(x!=0, arr.ind=TRUE)
  
  rownames(ccoef) <- NULL # just to be safe
  indexes <- apply(ccoef, 2, f)
  index.list <- as.list(indexes) # probably not necessary
  return(index.list)
}

```




```{r optimize6_sparse, echo=TRUE}
# we need to create a sparse Jacobian that will be in the inputs list and
# we need specify its structure in the ipoptr call:
#  - a list with 1 element per constraint
#  -- each element of the list is a vector with values that are the column indexes of the relevant constraints

opts$output_file <- paste0(PROJHOME, "/results/prob6.out")
inputs$jac_vector <- jac_flatten(inputs$ccoef, nz=TRUE) # now we keep only the nonzero elements of the Jacobian
eval_jac_g_structure_sparse <- make.sparse.structure(inputs$ccoef) # define the sparsity structure

result <- ipoptr(x0 = rep(1, length(inputs$weights)),
                 lb = xlb,
                 ub = xub,
                 eval_f = eval_f_xtop, # arguments: x, inputs
                 eval_grad_f = eval_grad_f_xtop,
                 eval_g = eval_g_dense, # constraints LHS - a vector of values
                 eval_jac_g = eval_jac_g,
                 eval_jac_g_structure = eval_jac_g_structure_sparse,
                 eval_h = eval_h_xtop, # the hessian is essential for this problem
                 eval_h_structure = eval_h_structure,
                 constraint_lb = clb,
                 constraint_ub = cub,
                 opts = opts,
                 inputs = inputs)

result$status; result$message; result$iterations
result$objective
quantile(result$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

calcsums <- calc_constraints(inputs$ccoef, inputs$constraint_vars, x=result$solution)

cbind(filesums, targets, clb, calcsums, cub) %>%
  kable(digits=0, format.args = list(big.mark=","))

result6 <- result

# compare objective function values
result1$objective; result2$objective; result3$objective; result4$objective; result5$objective; result6$objective

# compare iterations - scaling was a big improvement
result1$iterations; result2$iterations; result3$iterations; result4$iterations; result5$iterations; result6$iterations

```

## Set the problem up from the beginning with sparse matrix techniques - useful for HUGE problems
```{r optimize7_sparse_setup, echo=TRUE}
# calculate constraint coefficients and constraint values without ever using a dense matrix

# here we work through the data space and keep only the constraint coefficients that are nonzero
# ccvars <- c("E00100", "E00200", "E02000")
# ccoef_all <- pufsub %>%
#   mutate_at(vars(ccvars), 
#             list(npos = ~npos(., weight),
#                  sumval = ~sumval(., weight),
#                  nneg = ~nneg(., weight)))
# # glimpse(ccoef_all)
# ht(ccoef_all)

# we want the same constraints as before:
# E00100_npos
# E00100_sumval
# E00200_sumval
# E02000_nneg
# wage1, wage2, wage3

# our "cut" statement for wages used cut points as follows:
#    wagegroup=cut(E00200, c(-Inf, 0, 10e3, 50e3, 100e3, Inf)
# yielding these endpoints 
# wage1 (-Inf,0]
# wage2 (0,1e+04]
# wage3 (1e+04,5e+04]
# wage4 (5e+04,1e+05]
# wage5 (1e+05, Inf]

# create a "recipe" that says which ccoef's we want
recipe <- read_csv("
vname, valgroup, fn
E00100, TRUE, npos
E00100, TRUE, sumval
E00200, TRUE, sumval
E02000, TRUE, nneg

E00200, E00200<=0, sumval
E00200, E00200>0 & E00200<=10e3, sumval
E00200, E00200>10e3 & E00200<=50e3, sumval
")
recipe <- recipe %>%
  mutate(constraint_sort=row_number(),
         constraint_name=paste(vname, str_remove_all(valgroup, " "), fn, sep="_")) %>%
  select(constraint_sort, everything())
recipe

```


Now we can create a sparse data frame of constraint coefficients

```{r ccoef_sparse_function, echo=TRUE}
get_ccoefs <- function(recipe, puf){
  # recipe as passed to this function will have only one record - 
  #   that is, it has rules for a single constraint
  recipe <- as_tibble(recipe) # force this - otherwise passed as list
  pufuse <- puf %>%
    mutate(row_num=row_number()) %>%
    filter(eval(parse(text=recipe$valgroup))) %>%
    mutate(vname=recipe$vname) %>%
    select(vname, row_num, weight, value=recipe$vname)
  ccoef <- left_join(recipe, pufuse) %>%
    mutate(ccoef=case_when(recipe$fn=="sumval" ~ value * weight,
                           recipe$fn=="npos" ~ (value >0) * weight,
                           recipe$fn=="nneg" ~ (value < 0) * weight)) %>%
    # keep only the nonzero coefficients
    filter(ccoef!=0)
  return(ccoef)
}

```



```{r}
ccoef_sparse <- recipe %>%
  rowwise() %>%
  do(get_ccoefs(., pufsub)) %>%
    ungroup %>%
  # ONLY AFTER all filtering is done do we assign constraint numbers, as some
  # may drop out if they have no nonzero coefficients %>%
  mutate(constraint_num=group_indices(., constraint_sort)) %>%
  arrange(constraint_num, row_num) %>%
  select(constraint_num, everything())
# did any constraints drop out? yes
count(ccoef_sparse, constraint_num, constraint_sort, constraint_name)

```



```{r optimize7_constraint_function, echo=TRUE}
# now we need a function to calculate the constraints at a given point x
eval_g_sparse <- function(x, inputs){ 
  #.. constraints that must hold in the solution ----
  # just give the LHS of the expression
  # return a vector where each element evaluates a constraint (i.e., sum of (x * a cc matrix column), for each column)
  
  # ipoptr requires that ALL functions receive the same arguments, so a list called inputs is passed to ALL functions
  
  # This is the dense version in that it uses a dense set of constraint coefficients, inputs$ccoef (passed to this
  # function as a data frame but we could easily use a matrix instead). It has:
  #   one column for every constraint
  #   one row for every variable
  # Many of the cells are likely to be zero.
  
  constraint_vals <- inputs$ccoef_sparse %>%
    mutate(x=x[row_num]) %>%
    group_by(constraint_num) %>%
    summarise(constraint_val=sum(x * ccoef))
  
  return(constraint_vals$constraint_val)
}

# test the function
# inputs <- list(); inputs$ccoef_sparse <- ccoef_sparse
# x <- rep(1, nrow(pufsub))
# eval_g_sparse(x, inputs)
# filesums # do we get the same sums as before? yes, but wage0 drops out

```


Create functions to define the sparse Jacobian structure. In this case our Jacobian matrix is virtual or imaginary - we never created a dense matrix - but we need to define how each element of the long vector of constraint coefficients corresponds to this imaginary Jacobian matrix. That is, for each element of the vector, we need to know the indexes of the corresponding row (i.e., which x element) and corresponding column (i.e., which constraint) of the Jacobian.


+`jac_flatten_sparse` creates a long vector of nonzero constraint coefficients from our constraint coefficients data frame `ccoef_sparse`

+ `define_jac_g_structure_sparse` returns the sparsity structure. It returns a list with 1 element for each constraint. Each element is a vector of row indexes for the rows (x elements) that enter into the constraint calculation.



```{r}
# now we need to flatten the constraint coefficients
jac_flatten_sparse <- function(ccoef_sparse){
  ccoef_sparse %>%
    ungroup %>%
    arrange(constraint_num, row_num) %>%
    .[["ccoef"]]
}


define_jac_g_structure_sparse <- function(ccoef_sparse){
  #.. function to define the structure of the Jacobian ---
  # list with n_constraints elements
  # each is a vector indicating row indexes relevant to the constraint
  f <- function(cnum, ccoef_sparse) {
    ccoef_sparse %>% 
      filter(constraint_num==cnum) %>%
      .[["row_num"]]
  }
  jac_g_structure <- llply(unique(ccoef_sparse$constraint_num),
                           f,
                           ccoef_sparse=ccoef_sparse)
  return(jac_g_structure)
}
# Examples of how to use define_jac_g_structure_sparse:
# define_jac_g_structure_sparse(ccoef_sparse)

```


```{r optimize7_run, echo=TRUE}
x0 <- rep(1, nrow(pufsub))

# define targets and clb cub
# old targets 
targets
count(ccoef_sparse, constraint_num, constraint_sort, constraint_name)

# drop the one we don't need and remove names
targets_sparse <- targets[setdiff(names(targets), "wage1")] %>% unname
input_check <- list(); input_check$ccoef_sparse <- ccoef_sparse
init_vals <- eval_g_sparse(x0, input_check)

cbind(targets_sparse, init_vals, ratio=targets_sparse / init_vals)


# we have some different items to put into inputs
inputs <- list()
inputs$p <- 2
inputs$weights <- pufsub$weight
inputs$ccoef_sparse <- ccoef_sparse
inputs$jac_vector <- jac_flatten_sparse(inputs$ccoef_sparse)

# create vectors with constraint lower bounds and upper bounds - if they are the same, we have equality constraints
# create vectors with constraint lower bounds and upper bounds
# define tolerance - for simplicity, use the same for all, but that is not necessary
tol <- .01 # 1% range
clb <- targets_sparse
clb <- clb - tol * abs(clb)

cub <- targets_sparse
cub <- cub + tol * abs(cub)

cbind(init_vals, targets_sparse, clb, cub)

eval_jac_g_structure_sparse <- define_jac_g_structure_sparse(inputs$ccoef_sparse)

eval_h_structure <- hess_structure(length(inputs$weight))


# set arbitrary bounds for x that fall inside the solution above
xlb <- rep(.3, length(inputs$weights))
xub <- rep(10, length(inputs$weights))

opts <- list("print_level" = 0,
             "file_print_level" = 5, # integer
             "linear_solver" = "ma27", # mumps pardiso ma27 ma57 ma77 ma86 ma97
             "max_iter"=200)

opts$output_file <- paste0(PROJHOME, "/results/prob7.out")
opts$obj_scaling_factor <- 1e-4 # default 1 
opts$nlp_scaling_max_gradient <- 100 # default 100 -- not changing this now
opts  



result <- ipoptr(x0 = x0,
                 lb = xlb,
                 ub = xub,
                 eval_f = eval_f_xtop, # arguments: x, inputs
                 eval_grad_f = eval_grad_f_xtop,
                 eval_g = eval_g_sparse, # constraints LHS - a vector of values
                 eval_jac_g = eval_jac_g,
                 eval_jac_g_structure = eval_jac_g_structure_sparse,
                 eval_h = eval_h_xtop, # the hessian is essential for this problem
                 eval_h_structure = eval_h_structure,
                 # the constraint bounds are given here
                 constraint_lb = clb,
                 constraint_ub = cub,
                 opts = opts,
                 inputs = inputs)

result$status; result$message; result$iterations
result$objective
quantile(result$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

calcsums <- eval_g_sparse(x=result$solution, inputs)

cbind(init_vals, targets_sparse, clb, calcsums, cub) %>%
  kable(digits=0, format.args = list(big.mark=","))

result7 <- result

# compare objective function values
result1$objective; result2$objective; result3$objective; result4$objective; result5$objective; result6$objective; result7$objective

# compare iterations - scaling was a big improvement
result1$iterations; result2$iterations; result3$iterations; result4$iterations; result5$iterations; result6$iterations; result7$iterations

```

## Now try a different objective function - a differentiable approximation to absolute value

TaxData documentation says they minimize the sum of the absolute value of the difference between x and 1 (without taking into account the size of the current weight -  i.e., not weighting by that weight). We can create a differentiable approximation of this that can be used within IPOPT. 

A good twice-differentiable approximation of the absolute value of a variable $z$, $abs(v)$, is $(z^2 + s^2)^{1/2}$ where $s$ is a small number near zero (e.g., 0.01). This smooths the function around its minimum. In our case, $z$ is $x - 1$ and so the objective is:

$$min \sum_{x} [(x-1)^2 + s^2]^{1/2}$$

The first derivative of this for any $x_i$ is:
$$\frac{(x_i-1)}{[(x_i-1)^2+s^2]^{1/2}}$$


The second derivative is:

$$\frac{s^2}{((x_i-1)^2 + s^2)^{3/2}}$$


We implement this in a new objective function, gradient function, and hessian function for ipoptr, below.

```{r functions_absapprox, echo=TRUE}
eval_f_absapprox <- function(x, inputs) {
  #.. objective function - evaluates to a single number ----
  # returns a single value
  
  # ipoptr requires that ALL functions receive the same arguments, so a list called inputs is passed to ALL functions
  
  # here are the objective function, the 1st deriv, and the 2nd deriv
  # http://www.derivative-calculator.net/
  
  # [(x-1)^2 + s^2]^{1/2}                                objective function
  # (x - 1) / ({(x - 1)^2 + s^2}^(1/2))                  first deriv
  # s^2 / [(x-1)^2 + s^2]^(3/2)                          second deriv
  
  # make it easier to read:
  s <- inputs$s
  
  obj <- sum({(x-1)^2 + s^2}^(1/2))
  
  return(obj)
}


eval_grad_f_absapprox <- function(x, inputs){
  #.. gradient of objective function - a vector length x ----
  # giving the partial derivatives of obj wrt each x[i]
  # returns one value per element of x
  
  # ipoptr requires that ALL functions receive the same arguments, so a list called inputs is passed to ALL functions
  
  # here are the objective function, the 1st deriv, and the 2nd deriv
  # http://www.derivative-calculator.net/
  # 
  # [(x-1)^2 + s^2]^{1/2}                                objective function
  # (x - 1) / ({(x - 1)^2 + s^2}^(1/2))                  first deriv
  # s^2 / [(x-1)^2 + s^2]^(3/2)                          second deriv
  
  # make it easier to read:
  s <- inputs$s
  
  gradf <- (x - 1) / ({(x - 1)^2 + s^2}^(1/2))
  
  return(gradf)
}

eval_h_absapprox <- function(x, obj_factor, hessian_lambda, inputs){
  # The Hessian matrix ----
  # The Hessian matrix has many zero elements and so we set it up as a sparse matrix
  # We only keep the (potentially) non-zero values that run along the diagonal.
  # the Hessian is returned as a long vector. 
  # Separately, we define which elements of this vector correspond to which cells of the Hessian matrix.
  
  # obj_factor and hessian_lambda are required arguments of the function. They are created within ipoptr - we do not create them.
  
  # http://www.derivative-calculator.net/
  
  # [(x-1)^2 + s^2]^{1/2}                                objective function
  # (x - 1) / ({(x - 1)^2 + s^2}^(1/2))                  first deriv
  # s^2 / [(x-1)^2 + s^2]^(3/2)                          second deriv
  
  # make it easier to read:
  s <- inputs$s
  
  hess <- obj_factor * 
    ( s^2 / {((x-1)^2 + s^2)^(3/2)} )
  
  return(hess)
}

```


Now we rerun the same optimization with this new objective function and its gradient.
It can need more iterations, so I increase max_iter. 
I change the obj_scaling_factor to 10, which improves performance.
Finally, I experiment with different linear solvers; ma27 (default) works about as well as ma57 and ma86. Mumps and ma97 are a bit slower.

```{r optimize8_run, echo=TRUE}
# x0, targets, constraints, and tolerances have not changed absapprox
# nor have the x bounds, jacobian structure, nor most options for ipoptr

# we need a new element in the inputs list, s
inputs$s <- .01
ns(inputs)

opts$output_file <- paste0(PROJHOME, "/results/prob8.out")

# opts$obj_scaling_factor <- 1e-4 # default 1 
opts$obj_scaling_factor <- 1e1 # not sure what a good scaling factor should be, let' reset to the default
# opts$nlp_scaling_max_gradient <- 100 # default 100 -- not changing this now
opts$linear_solver <- "ma27"

opts$max_iter <- 500 # this objective function needs more iterations
opts  

# note the new function names below
result <- ipoptr(x0 = x0,
                 lb = xlb,
                 ub = xub,
                 eval_f = eval_f_absapprox, # arguments: x, inputs
                 eval_grad_f = eval_grad_f_absapprox,
                 eval_g = eval_g_sparse, # constraints LHS - a vector of values
                 eval_jac_g = eval_jac_g,
                 eval_jac_g_structure = eval_jac_g_structure_sparse,
                 eval_h = eval_h_absapprox, # the hessian is essential for this problem
                 eval_h_structure = eval_h_structure,
                 # the constraint bounds are given here
                 constraint_lb = clb,
                 constraint_ub = cub,
                 opts = opts,
                 inputs = inputs)

result$status; result$message; result$iterations
result$objective
quantile(result$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

calcsums <- eval_g_sparse(x=result$solution, inputs)

cbind(init_vals, targets_sparse, clb, calcsums, cub) %>%
  kable(digits=0, format.args = list(big.mark=","))

result8 <- result

# compare objective function values - here use the old objective function applied to this 
result1$objective; result2$objective; result3$objective; result4$objective; result5$objective; result6$objective; result7$objective; eval_f_xtop(result8$objective, inputs)

# compare iterations - scaling was a big improvement
result1$iterations; result2$iterations; result3$iterations; result4$iterations; result5$iterations; result6$iterations; result7$iterations; result8$iterations

```

# Now compare the distributions of x values for the 2 different objective functions

The distributions are quite different, at least by a simple examination of quantiles, with the x values generally in a smaller range around 1.

It's not clear how to interpret the differences given that the original weight enters into one objective function and not the other.

```{r compare_twoobj, echo=TRUE}
# compare quantiles
quantile(result7$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))
quantile(result8$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

```


  
# Put it all together to solve a realistic problem
Using the functions we've already created, we'll use the full PUF and set targets by AGI range for several variables.

```{r ONETIME_getdata_full, echo=TRUE, eval=FALSE}
pufraw <- read_csv(paste0(pufdir, "puf2011.csv"), 
                   col_types = cols(.default= col_double()))
# glimpse(pufraw)
ns(pufraw)

# keep a few variables:
# RECID unique id
# E00100 AGI
# E00200 wages
# E00300 interest received -- not kept
# E01700 pension income in AGI
# E02000 schedule E net income 
# S006 record weight as an integer - must be divided by 100
vars <- c("RECID", "E00100", "E00200", "E01700", "E02000", "S006")
puf_full <- pufraw %>% 
  select(vars) %>%
  mutate(weight=S006 / 100, otheragi=E00100 - E00200 - E01700 - E02000) %>%
  select(-S006) %>%
  select(RECID, weight, everything())

# glimpse(pufsub)
# ht(pufsub)
saveRDS(puf_full, paste0(PROJHOME, "/data/puf_full.rds")) # make sure data folder is in .gitignore

```


Define constraint coefficients.

```{r ccoef_full, echo=TRUE}
puf_full <- readRDS(paste0(PROJHOME, "/data/puf_full.rds"))

# create a recipe that says which ccoef's we want.
# As before, we want a df with 3 columns:
#   vname: a character variable name
#   valgroup: a character variable with a logical expression defining the data subgroup
#   fn: a charactervariable giving a function name (or calculation label)
#       right now, the function get_ccoefs has definitions for sumval, npos, and nnegs
#       so that's what we'll use, but we could expand it of course

# create a sub-recipe for file totals - must make sure valgroup is character
totals_recipe <- read_csv("
vname, valgroup, fn
E00100, TRUE, npos
E00100, TRUE, sumval
E00200, TRUE, sumval
E02000, TRUE, nneg", col_types = cols(.default=col_character()))

# create a sub-recipe for agi ranges
# Here we'll define agi ranges separately, for convenience
# there may be more efficient (less labor-intensive) ways to do this...

# 5 agi ranges
agiranges <- read_csv("agirange
E00100<=0
E00100>0 & E00100<=50e3
E00100>50e3 & E00100<=100e3
E00100>100e3 & E00100<=500e3
E00100>500e3")
agiranges

vnames <- c("E00100", "E00200", "E01700", "E02000") # 4 variables
fns <- c("sumval", "npos", "nneg") # 3 functions

# We are setting things up to define 5 x 4 x 3 = 60 constraints. Here goes:

range_recipe <- expand_grid(vname=vnames, valgroup=agiranges$agirange, fn=fns)
range_recipe

# now combine the two sub recipes
recipe <- bind_rows(totals_recipe, range_recipe) %>%
  mutate(constraint_sort=row_number(),
         constraint_name=paste(vname, str_remove_all(valgroup, " "), fn, sep="_")) %>%
  select(constraint_sort, everything())
recipe

ccoef_sparse <- recipe %>%
  rowwise() %>%
  do(get_ccoefs(., puf_full)) %>%
    ungroup %>%
  # ONLY AFTER all filtering is done do we assign constraint numbers, as some
  # may drop out if they have no nonzero coefficients %>%
  mutate(constraint_num=group_indices(., constraint_sort)) %>%
  arrange(constraint_num, row_num) %>%
  select(constraint_num, everything())
# did any constraints drop out? yes, quite a few
count(ccoef_sparse, constraint_num, constraint_sort, constraint_name)

```

Note that we defined 64 constraints but only 49 of them have nonzero constraint coefficients. Some things dropped out. For example, in the sparse version there are no constraints for number of returns with negative wages because there are zero records where wages are negative. Thus, there are no possible adjustments to weights that would get us negative wages.

Now that we have a recipe let's create some targets that are slightly perturbed. In an actual analysis, we would have targets that we constructed from actual data or else from forecasts.

```{r targets_full, echo=TRUE}
x0 <- rep(1, nrow(puf_full)) # initial x value
# we need an inputs list
input_check <- list(); input_check$ccoef_sparse <- ccoef_sparse

init_vals <- eval_g_sparse(x0, input_check)
set.seed(1234); (perturb <- runif(n=length(init_vals), min=.95, max=1.05))
targets_sparse <- init_vals * perturb

cbind(targets_sparse, init_vals, ratio=targets_sparse / init_vals)


```

## Solve using our regular objective function
Now we can solve the problem. I did a few things to make it easy to solve. 

+ First, I limited the perturbation to be within +/- 5% of the true values on the file.
+ Second, I established inequality constraints below that are +/- 2% of the targets.
+ Third, I expanded the bounds on the x values to range from 0.2 to 30
+ Fourth, in the options below I switched from the default solver ma27 to a more-appropriate solver, ma57, that requires a license (which I have).

Also, I adjusted the objective function scaling because this is a larger problem, but I doubt that was necessary.

On my machine an optimal solution is found in less than a minute. By contrast, I have been told that the TaxData reweighting, with far fewer targets, takes many hours.

```{r optimize_full, echo=TRUE}
inputs_full <- list()
inputs_full$p <- 2
inputs_full$weights <- puf_full$weight
inputs_full$ccoef_sparse <- ccoef_sparse
inputs_full$jac_vector <- jac_flatten_sparse(inputs_full$ccoef_sparse)

# create vectors with constraint lower bounds and upper bounds - if they are the same, we have equality constraints
# create vectors with constraint lower bounds and upper bounds
# define tolerance - for simplicity, use the same for all, but that is not necessary
tol <- .02 # 1% range
clb <- targets_sparse
clb <- clb - tol * abs(clb)

cub <- targets_sparse
cub <- cub + tol * abs(cub)

cbind(init_vals, targets_sparse, clb, cub)

eval_jac_g_structure_sparse <- define_jac_g_structure_sparse(inputs_full$ccoef_sparse)

eval_h_structure <- hess_structure(length(inputs_full$weight))


# set arbitrary bounds for x that fall inside the solution above
xlb <- rep(.2, length(inputs_full$weights))
xub <- rep(30, length(inputs_full$weights))

opts <- list("print_level" = 0,
             "file_print_level" = 5, # integer
             "max_iter"=200)
opts$linear_solver <- "ma57" # mumps pardiso ma27 ma57 ma77 ma86 ma97

opts$output_file <- paste0(PROJHOME, "/results/prob_full.out")
opts$obj_scaling_factor <- 1e-6 # default 1 
opts$nlp_scaling_max_gradient <- 100 # default 100 -- not changing this now
opts  

result <- ipoptr(x0 = x0,
                 lb = xlb,
                 ub = xub,
                 eval_f = eval_f_xtop, # arguments: x, inputs
                 eval_grad_f = eval_grad_f_xtop,
                 eval_g = eval_g_sparse, # constraints LHS - a vector of values
                 eval_jac_g = eval_jac_g,
                 eval_jac_g_structure = eval_jac_g_structure_sparse,
                 eval_h = eval_h_xtop, # the hessian is essential for this problem
                 eval_h_structure = eval_h_structure,
                 # the constraint bounds are given here
                 constraint_lb = clb,
                 constraint_ub = cub,
                 opts = opts,
                 inputs = inputs_full)

result_full <- result
result$status; result$message; result$iterations
result$objective
quantile(result$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

calcsums <- eval_g_sparse(x=result$solution, inputs)

cbind(init_vals, targets_sparse, clb, calcsums, cub) %>%
  kable(digits=0, format.args = list(big.mark=","))

```


## Repeat with the approximate absolute value function
```{r optimize_full_abs, echo=TRUE}
inputs_full <- list()
inputs_full$p <- 2
inputs_full$s <- .01
inputs_full$weights <- puf_full$weight
inputs_full$ccoef_sparse <- ccoef_sparse
inputs_full$jac_vector <- jac_flatten_sparse(inputs_full$ccoef_sparse)

# create vectors with constraint lower bounds and upper bounds - if they are the same, we have equality constraints
# create vectors with constraint lower bounds and upper bounds
# define tolerance - for simplicity, use the same for all, but that is not necessary
tol <- .02 # 1% range
clb <- targets_sparse
clb <- clb - tol * abs(clb)

cub <- targets_sparse
cub <- cub + tol * abs(cub)

cbind(init_vals, targets_sparse, clb, cub)

eval_jac_g_structure_sparse <- define_jac_g_structure_sparse(inputs_full$ccoef_sparse)

eval_h_structure <- hess_structure(length(inputs_full$weight))


# set arbitrary bounds for x that fall inside the solution above
xlb <- rep(.2, length(inputs_full$weights))
xub <- rep(30, length(inputs_full$weights))

opts <- list("print_level" = 0,
             "file_print_level" = 5, # integer
             "max_iter"=200)
opts$linear_solver <- "ma57" # mumps pardiso ma27 ma57 ma77 ma86 ma97

opts$output_file <- paste0(PROJHOME, "/results/prob_full_abs.out")
# opts$obj_scaling_factor <- 1e-6 # default 1 
opts$obj_scaling_factor <- 10 # not sure what a good scaling factor should be, let' reset to the default
opts$nlp_scaling_max_gradient <- 10 # default 100
opts  

# best combination of obj_scaling_factor and scaling_max_gradient, for iter:
# ma57:
#  10, 1, 150
# 10, 10, 131, 82 secs, ma57
# 10, 10, 155, 327 secs, ma86
# 10, 10, 162, 155 secs, ma97
# 100, 100, 200 max, ma57
# 100, 1, 200 max
# 10, 100, 200 max
# 100, 10, 200 max


result <- ipoptr(x0 = x0,
                 lb = xlb,
                 ub = xub,
                 eval_f = eval_f_absapprox, # arguments: x, inputs
                 eval_grad_f = eval_grad_f_absapprox,
                 eval_g = eval_g_sparse, # constraints LHS - a vector of values
                 eval_jac_g = eval_jac_g,
                 eval_jac_g_structure = eval_jac_g_structure_sparse,
                 eval_h = eval_h_absapprox, # the hessian is essential for this problem
                 eval_h_structure = eval_h_structure,
                 # the constraint bounds are given here
                 constraint_lb = clb,
                 constraint_ub = cub,
                 opts = opts,
                 inputs = inputs_full)

result_full_abs <- result
result$status; result$message; result$iterations
result$objective

quantile(result$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))
quantile(result_full_abs$solution, probs=c(0, .05, .1, .25, .5, .75, .9, .95, 1))

calcsums <- eval_g_sparse(x=result$solution, inputs)

cbind(init_vals, targets_sparse, clb, calcsums, cub) %>%
  kable(digits=0, format.args = list(big.mark=","))

```
  
  
# TODO: more-complex problems and related complexities

Among other things:

+ How to read IPOPT output
+ Troubleshooting problems that are hard to solve
+ Running massive problems in parallel (may never need this)



